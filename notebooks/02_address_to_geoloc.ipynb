{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Addresses to Longitude and Latitude\n",
    "SUUMO.jp gives us addresses for each property. To convert it to spacial data, longitude and latitude are obtained each chome in each of the 23 inner wards of Tokyo from https://nlftp.mlit.go.jp/cgi-bin/isj/dls/_view_cities_wards.cgi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ward data\n",
    "Combine the CSV longitude and latitude data from each ward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/app/data/geolocation.csv\"):\n",
    "\n",
    "    def combine_csv_files(directory_path, output_file):\n",
    "        base_path = Path(directory_path)\n",
    "\n",
    "        dataframes = []\n",
    "\n",
    "        # Iterate over each CSV\n",
    "        for csv_file in base_path.rglob('*.csv'):\n",
    "            df = pd.read_csv(csv_file, encoding='shift_jis')\n",
    "            dataframes.append(df)\n",
    "\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        combined_df.to_csv(output_file, index=False, encoding='shift_jis')\n",
    "\n",
    "        print(f\"All CSV files have been combined into {output_file}\")\n",
    "\n",
    "    # Combine CSV files from 'wards' folder into 'geolocation.csv'\n",
    "    combine_csv_files('/app/data/wards', '/app/data/geolocation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique addresses\n",
    "Get the unique addresses from the suumo property dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/data/sales_property_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/app/data/sales_property_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# df['address_block'] = df['address'].str.replace(r'[-－]\\d+[-－]\\d+', '', regex=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_block\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m'\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Extracting up to (and including) the first number\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/data/sales_property_data.csv'"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('/app/data/sales_property_data.csv')\n",
    "# df['address_block'] = df['address'].str.replace(r'[-－]\\d+[-－]\\d+', '', regex=True)\n",
    "df['address_block'] = df['address'].str.extract(r'(.*?\\d+)', expand=False)  # Extracting up to (and including) the first number\n",
    "df['address_block'] = df['address_block'].fillna(df['address']) # .extract returns NaN if no match, fill NaN with original address\n",
    "unique_addresses = df['address_block'].unique()\n",
    "print(f\"Total unique addresses: {len(unique_addresses)} of {len(df['address'])}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format suumo addresses\n",
    "Remove double width numbers and replace them with the chome format in the longitude and latitude data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      address_block converted_address\n",
      "0         東京都足立区花畑５       東京都足立区花畑五丁目\n",
      "1        東京都台東区松が谷３      東京都台東区松が谷三丁目\n",
      "2         東京都台東区下谷３       東京都台東区下谷三丁目\n",
      "3         東京都江東区南砂１       東京都江東区南砂一丁目\n",
      "4         東京都墨田区墨田２       東京都墨田区墨田二丁目\n",
      "...             ...               ...\n",
      "20967    東京都新宿区西新宿６      東京都新宿区西新宿六丁目\n",
      "20968    東京都渋谷区代々木５      東京都渋谷区代々木五丁目\n",
      "20969    東京都世田谷区玉川１      東京都世田谷区玉川一丁目\n",
      "20970      東京都港区白金６        東京都港区白金六丁目\n",
      "20971      東京都港区赤坂９        東京都港区赤坂九丁目\n",
      "\n",
      "[20972 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def arabic_to_kanji(num):\n",
    "    kanji_numerals = [\"\", \"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\"]\n",
    "    digits = [\"\", \"十\", \"百\", \"千\"]\n",
    "    kanji_string = \"\"\n",
    "    num_str = str(num)\n",
    "    position = 0\n",
    "\n",
    "    for digit in reversed(num_str):\n",
    "        if int(digit) > 0:\n",
    "            kanji_string = kanji_numerals[int(digit)] + digits[position] + kanji_string\n",
    "        position += 1\n",
    "\n",
    "    # Special case handling for numbers like 10, 20, 30, etc.\n",
    "    kanji_string = kanji_string.replace(\"一十\", \"十\")\n",
    "    \n",
    "    return kanji_string\n",
    "\n",
    "def convert_address_format(address):\n",
    "    import re\n",
    "    # Find all numbers in the address\n",
    "    numbers = re.findall(r'\\d+', address)\n",
    "    \n",
    "    for number in numbers:\n",
    "        kanji_number = arabic_to_kanji(int(number)) + '丁目'\n",
    "        # Replace the original number with its kanji representation\n",
    "        address = address.replace(number, kanji_number, 1)\n",
    "    \n",
    "    return address\n",
    "\n",
    "\n",
    "df['converted_address'] = df['address_block'].apply(convert_address_format)\n",
    "df['converted_address'] = df['converted_address'].str.strip()\n",
    "df['converted_address'] = df['converted_address'].str.replace('ヶ', 'ケ')\n",
    "print(df[['address_block', 'converted_address']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>都道府県コード</th>\n",
       "      <th>都道府県名</th>\n",
       "      <th>市区町村コード</th>\n",
       "      <th>市区町村名</th>\n",
       "      <th>大字町丁目コード</th>\n",
       "      <th>大字町丁目名</th>\n",
       "      <th>緯度</th>\n",
       "      <th>経度</th>\n",
       "      <th>原典資料コード</th>\n",
       "      <th>大字・字・丁目区分コード</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13121</td>\n",
       "      <td>足立区</td>\n",
       "      <td>131210001000</td>\n",
       "      <td>千住橋戸町</td>\n",
       "      <td>35.740193</td>\n",
       "      <td>139.797811</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13121</td>\n",
       "      <td>足立区</td>\n",
       "      <td>131210002001</td>\n",
       "      <td>千住緑町一丁目</td>\n",
       "      <td>35.742577</td>\n",
       "      <td>139.792502</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13121</td>\n",
       "      <td>足立区</td>\n",
       "      <td>131210002002</td>\n",
       "      <td>千住緑町二丁目</td>\n",
       "      <td>35.744936</td>\n",
       "      <td>139.792197</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13121</td>\n",
       "      <td>足立区</td>\n",
       "      <td>131210002003</td>\n",
       "      <td>千住緑町三丁目</td>\n",
       "      <td>35.747166</td>\n",
       "      <td>139.792460</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>東京都</td>\n",
       "      <td>13121</td>\n",
       "      <td>足立区</td>\n",
       "      <td>131210003000</td>\n",
       "      <td>千住河原町</td>\n",
       "      <td>35.743103</td>\n",
       "      <td>139.798787</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   都道府県コード 都道府県名  市区町村コード 市区町村名      大字町丁目コード   大字町丁目名         緯度          経度  \\\n",
       "0       13   東京都    13121   足立区  131210001000    千住橋戸町  35.740193  139.797811   \n",
       "1       13   東京都    13121   足立区  131210002001  千住緑町一丁目  35.742577  139.792502   \n",
       "2       13   東京都    13121   足立区  131210002002  千住緑町二丁目  35.744936  139.792197   \n",
       "3       13   東京都    13121   足立区  131210002003  千住緑町三丁目  35.747166  139.792460   \n",
       "4       13   東京都    13121   足立区  131210003000    千住河原町  35.743103  139.798787   \n",
       "\n",
       "   原典資料コード  大字・字・丁目区分コード  \n",
       "0        0             1  \n",
       "1        0             3  \n",
       "2        0             3  \n",
       "3        0             3  \n",
       "4        0             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/app/data/geolocation.csv'\n",
    "\n",
    "jp_geo_df = pd.read_csv(file_path, encoding='shift_jis')\n",
    "jp_geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jp_geo_df['combined'] = (jp_geo_df['都道府県名'].str.strip() + \n",
    "#                          jp_geo_df['市区町村名'].str.strip() + \n",
    "#                          jp_geo_df['大字町丁目名'].str.strip())\n",
    "# # column_values = jp_geo_df['combined'].tolist()\n",
    "# # column_values = [value for value in jp_geo_df['combined'] if '十' in value]\n",
    "# column_values = [value for value in jp_geo_df['combined'] if '東京都新宿区内' in value]\n",
    "\n",
    "# for c in column_values:\n",
    "#     print(c)\n",
    "# # print(column_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format addresses in geolocation and join\n",
    "The addresses in the geolocation data are split into 3 columns. First combine these columns so the addresses in the two datasets match and then merge the logitude and latitude columns into the original dataset using the matching addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>東京都足立区千住橋戸町</td>\n",
       "      <td>139.797811</td>\n",
       "      <td>35.740193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>東京都足立区千住緑町一丁目</td>\n",
       "      <td>139.792502</td>\n",
       "      <td>35.742577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>東京都足立区千住緑町二丁目</td>\n",
       "      <td>139.792197</td>\n",
       "      <td>35.744936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>東京都足立区千住緑町三丁目</td>\n",
       "      <td>139.792460</td>\n",
       "      <td>35.747166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>東京都足立区千住河原町</td>\n",
       "      <td>139.798787</td>\n",
       "      <td>35.743103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         address   longitude   latitude\n",
       "0    東京都足立区千住橋戸町  139.797811  35.740193\n",
       "1  東京都足立区千住緑町一丁目  139.792502  35.742577\n",
       "2  東京都足立区千住緑町二丁目  139.792197  35.744936\n",
       "3  東京都足立区千住緑町三丁目  139.792460  35.747166\n",
       "4    東京都足立区千住河原町  139.798787  35.743103"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Save only the address, longitude and latitude\n",
    "\n",
    "# jp_geo_df['combined'] = (jp_geo_df['都道府県名'].str.strip() + \n",
    "#                          jp_geo_df['市区町村名'].str.strip() + \n",
    "#                          jp_geo_df['大字町丁目名'].str.strip())\n",
    "\n",
    "# jp_geo_df.rename(columns={'緯度': 'latitude', '経度': 'longitude'}, inplace=True)\n",
    "\n",
    "# geo_only = jp_geo_df[[\"combined\", \"longitude\", \"latitude\"]]\n",
    "\n",
    "# geo_only = geo_only.rename(columns={\"combined\": \"address\"})\n",
    "\n",
    "# geo_only.to_csv('/app/data/add_long_lat.csv', index=False)\n",
    "\n",
    "# geo_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_geo_df['combined'] = (jp_geo_df['都道府県名'].str.strip() + \n",
    "                         jp_geo_df['市区町村名'].str.strip() + \n",
    "                         jp_geo_df['大字町丁目名'].str.strip())\n",
    "\n",
    "# Performing a left join on the address column\n",
    "result_df = pd.merge(df, jp_geo_df[['combined', '緯度', '経度']], left_on='converted_address', right_on='combined', how='left')\n",
    "result_df.rename(columns={'緯度': 'latitude', '経度': 'longitude'}, inplace=True)\n",
    "result_df.drop([\"combined\"], axis=1, inplace=True)\n",
    "\n",
    "# print(result_df[['converted_address', 'latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaN longitude and latitude\n",
    "The longitude and latitude are NaN in some cases. This is because the geolocation data does not cover some blocks. To deal with this, exclude the final number specifying the block. This results in a less accurate geolocation as the logitude and latitude are of a small area rather than the exact block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where latitude is NaN\n",
    "mask = result_df['latitude'].isnull()\n",
    "\n",
    "# Temporary extraction of modified addresses\n",
    "result_df['temp_address'] = result_df['address'].str.extract(r'(.*?)(?=\\d+)', expand=False)\n",
    "\n",
    "# Match the modified address and update latitude and longitude\n",
    "for idx, row in result_df.loc[mask].iterrows():\n",
    "    match = jp_geo_df[jp_geo_df['combined'] == row['temp_address']]\n",
    "    if not match.empty:\n",
    "        result_df.at[idx, 'latitude'] = match['緯度'].values[0]\n",
    "        result_df.at[idx, 'longitude'] = match['経度'].values[0]\n",
    "\n",
    "# Drop the temporary address column after use\n",
    "result_df.drop(columns=['temp_address'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the combined dataset\n",
    "Save the combined dataset and print out the number of missing values in latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have latitude and longitude.\n",
      "Missing latitude values: 0\n",
      "Missing longitude values: 0\n"
     ]
    }
   ],
   "source": [
    "result_df.to_csv('/app/data/sales_combo_property_geo.csv', index=False)\n",
    "\n",
    "# Check if any latitude or longitude values are missing\n",
    "missing_lat = result_df['latitude'].isna().any()\n",
    "missing_lon = result_df['longitude'].isna().any()\n",
    "\n",
    "if missing_lat or missing_lon:\n",
    "    print(\"There are missing values in latitude or longitude.\")\n",
    "else:\n",
    "    print(\"All rows have latitude and longitude.\")\n",
    "\n",
    "# Count missing values\n",
    "missing_lat_count = result_df['latitude'].isna().sum()\n",
    "missing_lon_count = result_df['longitude'].isna().sum()\n",
    "\n",
    "print(f\"Missing latitude values: {missing_lat_count}\")\n",
    "print(f\"Missing longitude values: {missing_lon_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
