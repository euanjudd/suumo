{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c888f424-8a66-4fa8-84b1-4827a22313d6",
   "metadata": {},
   "source": [
    "Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57d81e-a3fa-4660-9e92-a80bea2e2a19",
   "metadata": {},
   "source": [
    "# Scrape SUUMO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbb1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retry import retry # automatic retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # parse HTML\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d26645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=3, delay=10, backoff=2)\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Uses BeautifulSoup to parse HTML content from url. Retries 3 times with 10 second delay that doubles each try.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Will raise an exception for HTTP errors\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83757e6",
   "metadata": {},
   "source": [
    "## 1. Scrape URLs\n",
    "- Scrape URLs for individual purchasable property from SUUMO.jp collection pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "104eea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/app/data/used_property_urls.csv\"):\n",
    "    \n",
    "    collection_pages = {\n",
    "        \"used_apartments\": \"https://suumo.jp/jj/bukken/ichiran/JJ012FC001/?ar=030&bs=011&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13106&sc=13107&sc=13108&sc=13109&sc=13110&sc=13111&sc=13112&sc=13113&sc=13114&sc=13115&sc=13116&sc=13117&sc=13118&sc=13119&sc=13120&sc=13121&sc=13122&sc=13123&ta=13&po=0&pj=1&pc=100\",\n",
    "        # \"new_apartments\": \"https://suumo.jp/jj/bukken/ichiran/JJ011FC001/?ar=030&bs=010&ta=13&po=0&pj=1&initFlg=1&bknlistmodeflg=1&page=1&pc=100\",\n",
    "        \"used_houses\": \"https://suumo.jp/jj/bukken/ichiran/JJ012FC001/?ar=030&bs=021&cn=9999999&cnb=0&ekTjCd=&ekTjNm=&hb=0&ht=9999999&kb=1&kt=9999999&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&sc=13106&sc=13107&sc=13108&sc=13118&sc=13121&sc=13122&sc=13123&sc=13109&sc=13110&sc=13111&sc=13112&sc=13114&sc=13115&sc=13120&sc=13116&sc=13117&sc=13119&ta=13&tb=0&tj=0&tt=9999999&po=0&pj=1&pc=100\",\n",
    "        # \"new_houses\": \"https://suumo.jp/jj/bukken/ichiran/JJ012FC001/?ar=030&bs=020&ekTjCd=&ekTjNm=&hb=0&ht=9999999&kb=1&km=1&kt=9999999&kw=1&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&sc=13106&sc=13107&sc=13108&sc=13118&sc=13121&sc=13122&sc=13123&sc=13109&sc=13110&sc=13111&sc=13112&sc=13114&sc=13115&sc=13120&sc=13116&sc=13117&sc=13119&ta=13&tb=0&tj=0&tt=9999999&po=0&pj=1&pc=100\"\n",
    "    }\n",
    "\n",
    "    list_urls = []\n",
    "    page = 0\n",
    "\n",
    "    for category, url in collection_pages.items():\n",
    "        print(f\"Category: {category}\")\n",
    "\n",
    "        while True:\n",
    "            soup = get_html(url.format(page))\n",
    "            \n",
    "            if not soup:\n",
    "                print(f\"No soup in {url}\")\n",
    "                break \n",
    "\n",
    "            for h2 in soup.find_all('h2', class_='property_unit-title'):\n",
    "                a_tag = h2.find('a', href=True) \n",
    "                if a_tag and a_tag['href']:\n",
    "                    property_url = \"https://suumo.jp\" + a_tag['href']\n",
    "                    list_urls.append((category, property_url))\n",
    "\n",
    "            # Find the 'next' page link with text \"次へ\"\n",
    "            next_page = soup.find('a', text=\"次へ\")\n",
    "            if next_page and next_page.get('href'):\n",
    "                url = \"https://suumo.jp\" + next_page['href']\n",
    "            else:\n",
    "                print(f\"No more {category} pages.\")\n",
    "                break\n",
    "\n",
    "    df_urls = pd.DataFrame(list_urls, columns=['Category', 'URL'])\n",
    "    df_urls.to_csv(\"/app/data/used_property_urls.csv\", index=False)\n",
    "    df_urls.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a65362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2/25349 URLs\n",
      "Processed 4/25349 URLs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m list_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()  \u001b[38;5;66;03m# Manually release memory\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Rest 5 seconds to reduce server load\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parse_price(value):\n",
    "    try:\n",
    "        total_price = 0\n",
    "        price_str = value.strip().split('円')[0]\n",
    "        # 1億円\n",
    "        if \"億\" in price_str:\n",
    "            oku_pattern = re.compile(r'(\\d+)億')       # match int number before '億'\n",
    "            oku_match = oku_pattern.search(price_str)\n",
    "            if oku_match:\n",
    "                total_price += int(oku_match.group(1)) * 100000000\n",
    "        if \"万\" in price_str:\n",
    "            man_pattern = re.compile(r'(\\d+)万')\n",
    "            man_match = man_pattern.search(price_str)\n",
    "            if man_match:\n",
    "                total_price += int(man_match.group(1)) * 10000\n",
    "        if total_price:\n",
    "            return total_price\n",
    "        else:\n",
    "            return int(price_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def parse_area(value):\n",
    "    try:\n",
    "        area_pattern = re.compile(r'(\\d+(\\.\\d+)?)m')   # match decimal number before 'm'\n",
    "        area_match = area_pattern.search(value)\n",
    "        if area_match:\n",
    "            return float(area_match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_number(value, i):\n",
    "    try:\n",
    "        level_details = value.strip().split('/')\n",
    "        if len(level_details) > 1:\n",
    "            match = re.search(r'\\d+', level_details[i])\n",
    "            if match:\n",
    "                return int(match.group())\n",
    "        else: \n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def parse_year(value):\n",
    "    try:\n",
    "        return int(value.strip().split('年')[0])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\" Remove white space characters suchs as newline, tab, etc. \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def parse_features(soup):\n",
    "    \"\"\" Get soup of data in the 特徴ピックアップ section. \"\"\"\n",
    "    features_section = soup.find('h3', string='特徴ピックアップ')\n",
    "    if features_section:\n",
    "        parent_div = features_section.find_parent('div', class_='secTitleOuterR')\n",
    "        if parent_div == None:\n",
    "            parent_div = features_section.find_parent('div', class_='secTitleOuterK')\n",
    "        features_div = parent_div.find_next_sibling('div', class_='mt10')\n",
    "        if features_div:\n",
    "            features_text = ' '.join(features_div.stripped_strings)\n",
    "            return remove_whitespace(features_text)\n",
    "    return None\n",
    "\n",
    "pd_urls = pd.read_csv(\"/app/data/used_property_urls.csv\", index_col=False)\n",
    "df_urls = pd.DataFrame(pd_urls, columns=['Category', 'URL'])\n",
    "df_apt_urls = df_urls.loc[df_urls['Category'] == 'used_apartments']\n",
    "\n",
    "list_data = []\n",
    "\n",
    "count = 0\n",
    "batch_size = 100\n",
    "\n",
    "list_data = []\n",
    "\n",
    "if os.path.exists(\"/app/data/used_property_data.csv\"):\n",
    "    df_partial = pd.read_csv(f\"/app/data/used_property_data.csv\", index_col=False)\n",
    "    # Extract URL sets\n",
    "    urls_set1 = set(df_apt_urls['URL'])\n",
    "    urls_set2 = set(df_partial['URL'])\n",
    "    # URLs in df_apt_urls but not in df_partial\n",
    "    unique_urls1 = list(urls_set1 - urls_set2)\n",
    "else:\n",
    "    unique_urls1 = list(df_apt_urls['URL'])\n",
    "\n",
    "total = len(unique_urls1)\n",
    "\n",
    "for url in unique_urls1:\n",
    "    soup = get_html(url)\n",
    "\n",
    "    if not soup:\n",
    "        print(f\"No soup in {url}\")\n",
    "        continue \n",
    "\n",
    "    # Initialize dictionary to store the data\n",
    "    property_data = {}\n",
    "    property_data['url'] = url\n",
    "\n",
    "    # Iterate over each row in the table\n",
    "    target_table = soup.find('table', summary=\"表\")\n",
    "    for row in target_table.find_all('tr'):\n",
    "        # Find all th and td pairs in the row\n",
    "        ths = row.find_all('th')\n",
    "        tds = row.find_all('td')\n",
    "        \n",
    "        for th, td in zip(ths, tds):\n",
    "            header = th.text.strip()\n",
    "            value = td.text.strip()\n",
    "\n",
    "            if \"価格\" in header:\n",
    "                property_data['price'] = parse_price(value)\n",
    "            elif \"間取り\" in header:\n",
    "                property_data['plan'] = value.strip()\n",
    "            elif \"専有面積\" in header:\n",
    "                property_data['area'] = parse_area(value)\n",
    "            elif \"その他面積\" in header:\n",
    "                property_data['balcony_area'] = parse_area(value)\n",
    "            elif \"所在階/構造・階建\" in header:\n",
    "                property_data['level'] = extract_number(value, 0)\n",
    "                property_data['no_floors'] = extract_number(value, 1)\n",
    "            elif \"完成時期（築年月）\" in header:\n",
    "                property_data['year'] = parse_year(value)\n",
    "            elif \"住所\" in header:\n",
    "                property_data['address'] = value.strip().split('\\n')[0]\n",
    "\n",
    "    property_data['features'] = parse_features(soup)\n",
    "\n",
    "    list_data.append(property_data)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    # Periodically save property_data and clear the list from memory\n",
    "    if count % batch_size == 0:\n",
    "        print(f\"Processed {count}/{total} URLs\")\n",
    "        df_data = pd.DataFrame(list_data)\n",
    "        df_merged = pd.merge(df_apt_urls, df_data, left_on='URL', right_on='url', how='inner')\n",
    "        df_merged = df_merged.drop(columns=['url'])\n",
    "\n",
    "        if not os.path.exists(\"/app/data/used_property_data.csv\"):\n",
    "            # First batch: Write header and data\n",
    "            df_merged.to_csv(f'/app/data/used_property_data.csv', index=False)\n",
    "        else:\n",
    "            df_merged.to_csv(f'/app/data/used_property_data.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        property_data.clear()\n",
    "        list_data = []\n",
    "        gc.collect()  # Manually release memory\n",
    "        time.sleep(5)  # Rest 5 seconds to reduce server load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594618f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>URL</th>\n",
       "      <th>price</th>\n",
       "      <th>plan</th>\n",
       "      <th>area</th>\n",
       "      <th>balcony_area</th>\n",
       "      <th>level</th>\n",
       "      <th>no_floors</th>\n",
       "      <th>year</th>\n",
       "      <th>address</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>used_apartments</td>\n",
       "      <td>https://suumo.jp/ms/chuko/tokyo/sc_adachi/nc_7...</td>\n",
       "      <td>11900000</td>\n",
       "      <td>2DK</td>\n",
       "      <td>41.34</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1979</td>\n",
       "      <td>東京都足立区花畑５-2-9</td>\n",
       "      <td>年内引渡可 / 即引渡可 / 角住戸 / 陽当り良好 / 全居室収納 / シャワー付洗面化粧...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>used_apartments</td>\n",
       "      <td>https://suumo.jp/ms/chuko/tokyo/sc_taito/nc_75...</td>\n",
       "      <td>13800000</td>\n",
       "      <td>1K</td>\n",
       "      <td>16.45</td>\n",
       "      <td>2.69</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1991</td>\n",
       "      <td>東京都台東区松が谷３-６－２</td>\n",
       "      <td>瑕疵保証付（不動産会社独自） / ２沿線以上利用可 / 内装リフォーム / 駅まで平坦 / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Category                                                URL  \\\n",
       "0  used_apartments  https://suumo.jp/ms/chuko/tokyo/sc_adachi/nc_7...   \n",
       "1  used_apartments  https://suumo.jp/ms/chuko/tokyo/sc_taito/nc_75...   \n",
       "\n",
       "      price plan   area  balcony_area  level  no_floors  year         address  \\\n",
       "0  11900000  2DK  41.34          2.43      2          4  1979   東京都足立区花畑５-2-9   \n",
       "1  13800000   1K  16.45          2.69      5          9  1991  東京都台東区松が谷３-６－２   \n",
       "\n",
       "                                            features  \n",
       "0  年内引渡可 / 即引渡可 / 角住戸 / 陽当り良好 / 全居室収納 / シャワー付洗面化粧...  \n",
       "1  瑕疵保証付（不動産会社独自） / ２沿線以上利用可 / 内装リフォーム / 駅まで平坦 / ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.read_csv(f\"/app/data/used_property_data.csv\", index_col=False)\n",
    "df_merged.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
